{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f430cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nusla\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load dataset\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(df):\n",
    "    df = df.drop(columns=['Time'])  # Drop Time column\n",
    "    return df\n",
    "\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
    "\n",
    "# Handle class imbalance\n",
    "def balance_data(df):\n",
    "    minority_class = df[df['Class'] == 1]\n",
    "    majority_class = df[df['Class'] == 0].sample(n=len(minority_class) * 10, random_state=42)\n",
    "    balanced_df = pd.concat([minority_class, majority_class]).sample(frac=1, random_state=42)\n",
    "    return balanced_df\n",
    "\n",
    "data = balance_data(data)\n",
    "\n",
    "# Create initial graph data using k-NN approach\n",
    "def create_graph_data(df, k=5):\n",
    "    features = df.drop(columns=['Class']).values\n",
    "    labels = df['Class'].values\n",
    "    num_nodes = len(df)\n",
    "    dist_matrix = distance_matrix(features, features)\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes):\n",
    "        neighbors = np.argsort(dist_matrix[i])[1:k+1]  # Select k nearest neighbors\n",
    "        for n in neighbors:\n",
    "            edge_index.append([i, n])\n",
    "            edge_index.append([n, i])  # Make it bidirectional\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(features, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "graph_data = create_graph_data(data)\n",
    "\n",
    "def split_data(data, test_size=0.2):\n",
    "    num_nodes = data.x.shape[0]\n",
    "    train_idx, test_idx = train_test_split(np.arange(num_nodes), test_size=test_size, random_state=42, stratify=data.y.numpy())\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    data.train_mask = train_mask\n",
    "    data.test_mask = test_mask\n",
    "    return data\n",
    "\n",
    "graph_data = split_data(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee39eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0 -2.603909  2.200345 -2.535144  0.129397 -0.857817  1.202565 -2.429855   \n",
      "1 -0.438957  0.625134  1.293982  0.761969  0.392903 -0.289577  0.380345   \n",
      "2 -0.452042  0.834484  1.138837  0.539390 -0.197061 -0.175326  0.293780   \n",
      "3  1.156146 -0.378593 -1.698718 -0.718181  0.142121 -1.086934  0.233704   \n",
      "4  0.550839 -0.258247 -0.470772 -0.680758 -0.163034 -0.869149  0.407502   \n",
      "\n",
      "         7         8         9   ...        19         20        21        22  \\\n",
      "0 -7.238274  1.170055 -3.414142  ... -3.640181  11.273224 -3.854414  1.746642   \n",
      "1 -0.119400  0.539669 -0.535535  ... -0.067689  -0.610731 -1.549316 -0.323972   \n",
      "2  0.309625 -0.877996 -0.643262  ...  0.100321  -0.121969 -0.563403 -0.188127   \n",
      "3 -0.381931 -0.932955  1.065416  ... -0.191513   0.665382  1.884584 -0.441949   \n",
      "4 -0.225893  0.579333 -0.671154  ...  0.128638   0.151801  0.133996 -0.544871   \n",
      "\n",
      "         23        24        25        26        27        28  \n",
      "0 -0.262958  1.020870 -1.030898  2.337828  1.677097  0.691151  \n",
      "1 -0.380175  0.225869 -1.235738 -0.071740  0.161517 -0.269070  \n",
      "2  0.034799  0.467495 -0.974417 -0.078222  0.085468 -0.233327  \n",
      "3 -0.577926  1.390341  0.642385 -0.279039 -0.331529 -0.273268  \n",
      "4  0.046328  1.680500 -0.957805 -0.064167  0.043416  0.166282  \n",
      "\n",
      "[5 rows x 29 columns]    Class\n",
      "0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features (x) and labels (y) from the graph\n",
    "features = graph_data.x.numpy()  # Convert from torch tensor to numpy array\n",
    "labels = graph_data.y.numpy()    # Convert labels to numpy array\n",
    "\n",
    "# Create a DataFrame for better inspection (optional)\n",
    "features_df = pd.DataFrame(features)\n",
    "labels_df = pd.DataFrame(labels, columns=[\"Class\"])\n",
    "\n",
    "# Print the first few rows of the features and labels (optional)\n",
    "print(features_df.head(), labels_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (4329, 29)\n",
      "Testing features shape: (1083, 29)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, stratify=labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: You can print the shapes to confirm\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7233a76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.987072945521699\n",
      "Recall: 0.8775510204081632\n",
      "Precision: 0.9772727272727273\n",
      "F1 Score: 0.9247311827956989\n",
      "ROC-AUC: 0.9929555578576608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "y_prob = lr_model.predict_proba(X_test)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0660e442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Accuracy: 0.9879963065558633\n",
      "Recall: 0.8673469387755102\n",
      "Precision: 1.0\n",
      "F1 Score: 0.9289617486338798\n",
      "ROC-AUC: 0.9953744949756553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Random Forest Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(f\"Recall: {recall_rf}\")\n",
    "print(f\"Precision: {precision_rf}\")\n",
    "print(f\"F1 Score: {f1_rf}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25473a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Metrics:\n",
      "Accuracy: 0.9889196675900277\n",
      "Recall: 0.8877551020408163\n",
      "Precision: 0.9886363636363636\n",
      "F1 Score: 0.9354838709677419\n",
      "ROC-AUC: 0.9976691184087849\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]  # Probability scores for ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"XGBoost Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_xgb}\")\n",
    "print(f\"Recall: {recall_xgb}\")\n",
    "print(f\"Precision: {precision_xgb}\")\n",
    "print(f\"F1 Score: {f1_xgb}\")\n",
    "print(f\"ROC-AUC: {roc_auc_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05548f12",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
